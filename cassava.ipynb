{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T05:33:56.219900Z",
     "iopub.status.busy": "2025-03-21T05:33:56.219165Z",
     "iopub.status.idle": "2025-03-21T05:33:56.225973Z",
     "shell.execute_reply": "2025-03-21T05:33:56.224579Z",
     "shell.execute_reply.started": "2025-03-21T05:33:56.219862Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.nn.functional import softmax\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T05:33:58.674714Z",
     "iopub.status.busy": "2025-03-21T05:33:58.674243Z",
     "iopub.status.idle": "2025-03-21T05:33:58.680649Z",
     "shell.execute_reply": "2025-03-21T05:33:58.679425Z",
     "shell.execute_reply.started": "2025-03-21T05:33:58.674673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transform_inception = transforms.Compose([\n",
    "    transforms.Resize(320),\n",
    "    transforms.RandomResizedCrop(299),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_vit = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T05:34:00.943208Z",
     "iopub.status.busy": "2025-03-21T05:34:00.942777Z",
     "iopub.status.idle": "2025-03-21T05:34:00.948682Z",
     "shell.execute_reply": "2025-03-21T05:34:00.947323Z",
     "shell.execute_reply.started": "2025-03-21T05:34:00.943178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = './cassava'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "\n",
    "# Full datasets\n",
    "full_dataset_incep = datasets.ImageFolder(train_dir, transform=transform_inception)\n",
    "full_dataset_vit = datasets.ImageFolder(train_dir, transform=transform_vit)\n",
    "\n",
    "# Shared split\n",
    "dataset_size = len(full_dataset_incep)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "indices = torch.randperm(dataset_size)\n",
    "train_indices, val_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "# Subsets\n",
    "train_dataset_incep = Subset(full_dataset_incep, train_indices)\n",
    "val_dataset_incep = Subset(full_dataset_incep, val_indices)\n",
    "\n",
    "train_dataset_vit = Subset(full_dataset_vit, train_indices)\n",
    "val_dataset_vit = Subset(full_dataset_vit, val_indices)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader_incep = DataLoader(train_dataset_incep, batch_size=32, shuffle=True)\n",
    "val_loader_incep = DataLoader(val_dataset_incep, batch_size=32, shuffle=False)\n",
    "\n",
    "train_loader_vit = DataLoader(train_dataset_vit, batch_size=32, shuffle=True)\n",
    "val_loader_vit = DataLoader(val_dataset_vit, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T05:34:03.275369Z",
     "iopub.status.busy": "2025-03-21T05:34:03.274928Z",
     "iopub.status.idle": "2025-03-21T05:34:09.037463Z",
     "shell.execute_reply": "2025-03-21T05:34:09.036163Z",
     "shell.execute_reply.started": "2025-03-21T05:34:03.275338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class weights\n",
    "labels = [y for _, y in full_dataset_incep]\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(full_dataset_incep.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Inception] Epoch 1/30 | Train Loss: 1.4182 | Val Acc: 0.5194\n",
      "[Inception] Epoch 2/30 | Train Loss: 1.1607 | Val Acc: 0.6634\n",
      "[Inception] Epoch 3/30 | Train Loss: 1.0649 | Val Acc: 0.5186\n",
      "[Inception] Epoch 4/30 | Train Loss: 0.9968 | Val Acc: 0.6784\n",
      "[Inception] Epoch 5/30 | Train Loss: 0.9211 | Val Acc: 0.6475\n",
      "[Inception] Epoch 6/30 | Train Loss: 0.8856 | Val Acc: 0.6528\n",
      "[Inception] Epoch 7/30 | Train Loss: 0.8826 | Val Acc: 0.6201\n",
      "[Inception] Epoch 8/30 | Train Loss: 0.8667 | Val Acc: 0.6846\n",
      "[Inception] Epoch 9/30 | Train Loss: 0.8467 | Val Acc: 0.5424\n",
      "[Inception] Epoch 10/30 | Train Loss: 0.8135 | Val Acc: 0.7898\n",
      "[Inception] Epoch 11/30 | Train Loss: 0.7194 | Val Acc: 0.7871\n",
      "[Inception] Epoch 12/30 | Train Loss: 0.6914 | Val Acc: 0.7942\n",
      "[Inception] Epoch 13/30 | Train Loss: 0.6507 | Val Acc: 0.8030\n",
      "[Inception] Epoch 14/30 | Train Loss: 0.6213 | Val Acc: 0.8048\n",
      "[Inception] Epoch 15/30 | Train Loss: 0.6210 | Val Acc: 0.8092\n",
      "[Inception] Epoch 16/30 | Train Loss: 0.6271 | Val Acc: 0.7959\n",
      "[Inception] Epoch 17/30 | Train Loss: 0.6120 | Val Acc: 0.7933\n",
      "[Inception] Epoch 18/30 | Train Loss: 0.6035 | Val Acc: 0.8004\n",
      "[Inception] Epoch 19/30 | Train Loss: 0.5952 | Val Acc: 0.7898\n",
      "[Inception] Epoch 20/30 | Train Loss: 0.5942 | Val Acc: 0.8101\n",
      "[Inception] Epoch 21/30 | Train Loss: 0.5933 | Val Acc: 0.8083\n",
      "[Inception] Epoch 22/30 | Train Loss: 0.5725 | Val Acc: 0.8224\n",
      "[Inception] Epoch 23/30 | Train Loss: 0.5632 | Val Acc: 0.8286\n",
      "[Inception] Epoch 24/30 | Train Loss: 0.5615 | Val Acc: 0.8163\n",
      "[Inception] Epoch 25/30 | Train Loss: 0.5770 | Val Acc: 0.8163\n",
      "[Inception] Epoch 26/30 | Train Loss: 0.5739 | Val Acc: 0.8207\n",
      "[Inception] Epoch 27/30 | Train Loss: 0.5592 | Val Acc: 0.8021\n",
      "[Inception] Epoch 28/30 | Train Loss: 0.5491 | Val Acc: 0.8375\n",
      "[Inception] Epoch 29/30 | Train Loss: 0.5448 | Val Acc: 0.8012\n",
      "[Inception] Epoch 30/30 | Train Loss: 0.5323 | Val Acc: 0.8198\n"
     ]
    }
   ],
   "source": [
    "# InceptionV3\n",
    "model_incep = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "model_incep.fc = nn.Linear(model_incep.fc.in_features, num_classes)\n",
    "model_incep.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_incep = torch.optim.Adam(model_incep.parameters(), lr=0.001)\n",
    "scheduler_incep = torch.optim.lr_scheduler.StepLR(optimizer_incep, step_size=10, gamma=0.1)\n",
    "scaler_incep = GradScaler()\n",
    "\n",
    "best_val_acc_incep = 0\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_incep.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader_incep:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_incep.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model_incep(images)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler_incep.scale(loss).backward()\n",
    "        scaler_incep.step(optimizer_incep)\n",
    "        scaler_incep.update()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader_incep.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model_incep.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_incep:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_incep(images)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    if val_acc > best_val_acc_incep:\n",
    "        best_val_acc_incep = val_acc\n",
    "        torch.save(model_incep.state_dict(), 'best_model_inception.pth')\n",
    "\n",
    "    scheduler_incep.step()\n",
    "\n",
    "    print(f\"[Inception] Epoch {epoch+1}/30 | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T05:34:48.821187Z",
     "iopub.status.busy": "2025-03-21T05:34:48.820752Z",
     "iopub.status.idle": "2025-03-21T05:37:22.649526Z",
     "shell.execute_reply": "2025-03-21T05:37:22.647896Z",
     "shell.execute_reply.started": "2025-03-21T05:34:48.821155Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionV3 Final Accuracy: 0.818904593639576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.65      0.56        75\n",
      "           1       0.85      0.79      0.82       285\n",
      "           2       0.71      0.84      0.77       148\n",
      "           3       0.95      0.84      0.89       564\n",
      "           4       0.60      0.90      0.72        60\n",
      "\n",
      "    accuracy                           0.82      1132\n",
      "   macro avg       0.72      0.80      0.75      1132\n",
      "weighted avg       0.84      0.82      0.83      1132\n",
      "\n",
      "[[ 49   8   5   6   7]\n",
      " [ 34 226   7  10   8]\n",
      " [  3   6 124  10   5]\n",
      " [ 12  24  38 474  16]\n",
      " [  3   2   0   1  54]]\n"
     ]
    }
   ],
   "source": [
    "model_incep.load_state_dict(torch.load('best_model_inception.pth'))\n",
    "model_incep.eval()\n",
    "\n",
    "val_preds, val_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader_incep:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_incep(images)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"InceptionV3 Final Accuracy:\", accuracy_score(val_labels, val_preds))\n",
    "print(classification_report(val_labels, val_preds))\n",
    "print(confusion_matrix(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ViT] Epoch 1/30 | Train Loss: 1.5192 | Val Acc: 0.4611\n",
      "[ViT] Epoch 2/30 | Train Loss: 1.3745 | Val Acc: 0.5186\n",
      "[ViT] Epoch 3/30 | Train Loss: 1.2898 | Val Acc: 0.5583\n",
      "[ViT] Epoch 4/30 | Train Loss: 1.2361 | Val Acc: 0.5592\n",
      "[ViT] Epoch 5/30 | Train Loss: 1.1981 | Val Acc: 0.5707\n",
      "Unfroze ViT backbone.\n",
      "[ViT] Epoch 6/30 | Train Loss: 0.8763 | Val Acc: 0.7906\n",
      "[ViT] Epoch 7/30 | Train Loss: 0.6854 | Val Acc: 0.7279\n",
      "[ViT] Epoch 8/30 | Train Loss: 0.5932 | Val Acc: 0.7968\n",
      "[ViT] Epoch 9/30 | Train Loss: 0.5304 | Val Acc: 0.8074\n",
      "[ViT] Epoch 10/30 | Train Loss: 0.4763 | Val Acc: 0.8242\n",
      "[ViT] Epoch 11/30 | Train Loss: 0.4235 | Val Acc: 0.8534\n",
      "[ViT] Epoch 12/30 | Train Loss: 0.4078 | Val Acc: 0.8366\n",
      "[ViT] Epoch 13/30 | Train Loss: 0.3597 | Val Acc: 0.8525\n",
      "[ViT] Epoch 14/30 | Train Loss: 0.3132 | Val Acc: 0.8428\n",
      "[ViT] Epoch 15/30 | Train Loss: 0.2917 | Val Acc: 0.8454\n",
      "[ViT] Epoch 16/30 | Train Loss: 0.4767 | Val Acc: 0.8366\n",
      "[ViT] Epoch 17/30 | Train Loss: 0.4490 | Val Acc: 0.7818\n",
      "[ViT] Epoch 18/30 | Train Loss: 0.4344 | Val Acc: 0.7977\n",
      "[ViT] Epoch 19/30 | Train Loss: 0.4094 | Val Acc: 0.8525\n",
      "[ViT] Epoch 20/30 | Train Loss: 0.4045 | Val Acc: 0.8304\n",
      "[ViT] Epoch 21/30 | Train Loss: 0.3649 | Val Acc: 0.8269\n",
      "[ViT] Epoch 22/30 | Train Loss: 0.3144 | Val Acc: 0.8419\n",
      "[ViT] Epoch 23/30 | Train Loss: 0.3047 | Val Acc: 0.8595\n",
      "[ViT] Epoch 24/30 | Train Loss: 0.3084 | Val Acc: 0.8534\n",
      "[ViT] Epoch 25/30 | Train Loss: 0.2714 | Val Acc: 0.8322\n",
      "[ViT] Epoch 26/30 | Train Loss: 0.2502 | Val Acc: 0.8419\n",
      "[ViT] Epoch 27/30 | Train Loss: 0.2083 | Val Acc: 0.8754\n",
      "[ViT] Epoch 28/30 | Train Loss: 0.2057 | Val Acc: 0.8560\n",
      "[ViT] Epoch 29/30 | Train Loss: 0.1676 | Val Acc: 0.8445\n",
      "[ViT] Epoch 30/30 | Train Loss: 0.1665 | Val Acc: 0.8560\n"
     ]
    }
   ],
   "source": [
    "model_vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "model_vit.heads.head = nn.Linear(model_vit.heads.head.in_features, num_classes)\n",
    "model_vit.to(device)\n",
    "\n",
    "# Freeze backbone initially\n",
    "for param in model_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_vit.heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer_vit = AdamW(model_vit.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "scheduler_vit = CosineAnnealingWarmRestarts(optimizer_vit, T_0=10, T_mult=2)\n",
    "scaler_vit = GradScaler()\n",
    "criterion_vit = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "best_val_acc_vit = 0\n",
    "freeze_epochs = 5\n",
    "\n",
    "for epoch in range(30):\n",
    "    if epoch == freeze_epochs:\n",
    "        # Unfreeze backbone\n",
    "        for param in model_vit.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer_vit = AdamW(model_vit.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "        scheduler_vit = CosineAnnealingWarmRestarts(optimizer_vit, T_0=10, T_mult=2)\n",
    "        print(\"Unfroze ViT backbone.\")\n",
    "\n",
    "    model_vit.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader_vit:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_vit.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model_vit(images)\n",
    "            loss = criterion_vit(outputs, labels)\n",
    "        scaler_vit.scale(loss).backward()\n",
    "        scaler_vit.step(optimizer_vit)\n",
    "        scaler_vit.update()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_loader_vit.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model_vit.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_vit:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_vit(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    if val_acc > best_val_acc_vit:\n",
    "        best_val_acc_vit = val_acc\n",
    "        torch.save(model_vit.state_dict(), 'best_model_vit.pth')\n",
    "\n",
    "    scheduler_vit.step()\n",
    "\n",
    "    print(f\"[ViT] Epoch {epoch+1}/30 | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Final Accuracy: 0.8498233215547704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.79      0.60        75\n",
      "           1       0.87      0.78      0.82       285\n",
      "           2       0.82      0.74      0.78       148\n",
      "           3       0.94      0.91      0.93       564\n",
      "           4       0.75      0.93      0.83        60\n",
      "\n",
      "    accuracy                           0.85      1132\n",
      "   macro avg       0.77      0.83      0.79      1132\n",
      "weighted avg       0.87      0.85      0.85      1132\n",
      "\n",
      "[[ 59   8   5   1   2]\n",
      " [ 36 223   5  16   5]\n",
      " [ 11   9 109  13   6]\n",
      " [ 15  14  14 515   6]\n",
      " [  0   2   0   2  56]]\n"
     ]
    }
   ],
   "source": [
    "model_vit.load_state_dict(torch.load('best_model_vit.pth'))\n",
    "model_vit.eval()\n",
    "\n",
    "val_preds_vit, val_labels_vit = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader_vit:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_vit(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        val_preds_vit.extend(preds.cpu().numpy())\n",
    "        val_labels_vit.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"ViT Final Accuracy:\", accuracy_score(val_labels_vit, val_preds_vit))\n",
    "print(classification_report(val_labels_vit, val_preds_vit))\n",
    "print(confusion_matrix(val_labels_vit, val_preds_vit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ensemble Accuracy: 0.8683745583038869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.76      0.64        75\n",
      "           1       0.86      0.80      0.83       285\n",
      "           2       0.88      0.80      0.84       148\n",
      "           3       0.95      0.93      0.94       564\n",
      "           4       0.70      0.93      0.80        60\n",
      "\n",
      "    accuracy                           0.87      1132\n",
      "   macro avg       0.79      0.84      0.81      1132\n",
      "weighted avg       0.88      0.87      0.87      1132\n",
      "\n",
      "[[ 57   9   4   2   3]\n",
      " [ 32 228   3  17   5]\n",
      " [  4  10 119   8   7]\n",
      " [  8  14  10 523   9]\n",
      " [  1   3   0   0  56]]\n"
     ]
    }
   ],
   "source": [
    "# Load best models\n",
    "model_incep.load_state_dict(torch.load('best_model_inception.pth'))\n",
    "model_incep.eval()\n",
    "model_vit.load_state_dict(torch.load('best_model_vit.pth'))\n",
    "model_vit.eval()\n",
    "\n",
    "# Get softmax logits\n",
    "inception_logits = []\n",
    "vit_logits = []\n",
    "true_labels = []\n",
    "\n",
    "# Inception predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader_incep:\n",
    "        images = images.to(device)\n",
    "        outputs = model_incep(images)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "        inception_logits.append(softmax(outputs, dim=1).cpu())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# ViT predictions\n",
    "with torch.no_grad():\n",
    "    for images, _ in val_loader_vit:\n",
    "        images = images.to(device)\n",
    "        outputs = model_vit(images)\n",
    "        vit_logits.append(softmax(outputs, dim=1).cpu())\n",
    "\n",
    "# Stack\n",
    "inception_logits = torch.cat(inception_logits, dim=0)\n",
    "vit_logits = torch.cat(vit_logits, dim=0)\n",
    "\n",
    "# Weighted ensemble\n",
    "ensemble_logits = 0.6 * inception_logits + 0.4 * vit_logits\n",
    "ensemble_preds = torch.argmax(ensemble_logits, dim=1).numpy()\n",
    "\n",
    "# Evaluation\n",
    "print(\"Final Ensemble Accuracy:\", accuracy_score(true_labels, ensemble_preds))\n",
    "print(classification_report(true_labels, ensemble_preds))\n",
    "print(confusion_matrix(true_labels, ensemble_preds))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 422313,
     "sourceId": 13277,
     "sourceType": "competition"
    },
    {
     "datasetId": 1562973,
     "sourceId": 2574068,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3476702,
     "sourceId": 6073860,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
